what model to use
https://arxiv.org/pdf/2311.07911



REAL SOURCE USED STARTING HERE:
https://arxiv.org/pdf/2510.15682
    -original squai paper

https://arxiv.org/pdf/2105.03011 (about testdata)
    -only read title and abstract to formulate question 
    -question not answerable by title, but expected to be answered somewhere in the paper
    -if question answerable with the paper, answer it
    -select the minimal snippet necessary for answering the question in the paper
    -general questions / questions only answerable by a specific paper

    -information-veryfing paradigm where the writer of the question already knows the answer (questions solely written for evaluating the knowledge)
    -in contrast: information seeking paradigm

https://arxiv.org/pdf/2502.13668 (PEER QA)
    -dataset size: The size is in line with other recent
datasets such as HumanEval (Chen et al., 2021a)
(164 examples), TruthfulQA (Lin et al., 2022)
(817), and GPQA (Rein et al., 2024) (448). In


GENERATING THE TEST DATAAAAAA: (SciQAG)
https://arxiv.org/pdf/2405.09939


https://arxiv.org/pdf/2509.16952 (AIR QA)
-->Dataset: https://huggingface.co/datasets/OpenDFM/AirQA-Real


MAYBE LOOKS GOODÃ¥
https://huggingface.co/datasets/LinerAI/QASA/viewer/default/test (qasa dataset)

task type multiple papers:
SciDQA
M3SciQA
AirQA

open domain qa
-->current problem a lot of datasets are directly tied to an underlying paper
-->missing datasets

possible further improvements
-->squai can only handle text, no graphs or images



why use decontextualization instead of LLM generated questions like the ones in squai
  For a master's thesis, the scientific "Gold Standard" is Decontextualization.While generating synthetic questions (like the deepeval examples you found) is easier, it is scientifically weaker because it tests your system on "robot logic" rather than human intent.Here is the breakdown of why you should choose Decontextualization as your primary benchmark, and how to use synthetic data only as a backup.1. Scientific Comparison: Synthetic vs. DecontextualizedFeatureSynthetic Generation (DeepEval)Decontextualization (PeerQA + Rewriting)OriginGenerated from the text by an AI.Generated by a Human Expert (Reviewer).DifficultyLow/Artificial. The AI tends to use the same keywords as the document, making retrieval too easy.High/Realistic. Humans use synonyms, vague terms, and complex reasoning that require "semantic understanding".BiasHigh Distributional Bias. The questions mimic the document's structure, not user behavior.Low Bias. Represents real information-seeking behavior.Thesis ValueWeak. Proves your code runs, but not that it is "smart."Strong. Proves your system can handle the "Distribution Shift" between user queries and technical docs.2. Why "Decontextualization" is the Better ChoiceYour thesis is about SQuAI (a RAG system). The core challenge of RAG is not "finding a document," but "bridging the gap" between a user's question and the document's text.The Problem with Synthetic Data: If you use DeepEval to generate questions from the paper Attention Is All You Need, the AI might ask:"What are the attention mechanisms described in the 'Attention Is All You Need' paper?"This is too easy. It shares 80% of the keywords with the document. Your retrieval system will score 100% accuracy, but it doesn't mean it's intelligent.The Power of Decontextualization: A real human (from PeerQA) might ask:"How does this architecture parallelize compared to RNNs?"This doesn't mention "Attention" or the title.Decontextualizing it to "How does the Transformer architecture parallelize compared to RNNs?" creates a perfect test case. It forces SQuAI to understand that "Transformer" = "this architecture" and retrieve the right paper without dead-giveaway keywords.3. Recommended "Thesis Strategy"To make your thesis bulletproof, use both methods, but label them clearly:Method A: The "Stress Test" (Synthetic)Source: Use deepeval or your own script to generate 1,000+ questions from your unarxive data.Purpose: Use this to tune your hyperparameters (e.g., "Should I use chunk size 500 or 1000?").Thesis Claim: "We used synthetic data for large-scale parameter optimization following the methodology of Van Elburg et al. (2025)."Method B: The "Hero Benchmark" (Decontextualized)Source: Take 200-500 questions from PeerQA.Action: Run the "Decontextualization Pipeline" (using GPT-4 or decontext) to inject the paper titles.Purpose: This is your Final Evaluation Table.Thesis Claim: "For ecological validity, we evaluated the final model on human-generated expert questions from the PeerQA dataset, decontextualized using the CANARD framework to ensure standalone interpretability."