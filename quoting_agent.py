import json
from typing import Final
from util import buildJudgementPrompt, loadClaimContextPairs, printAndLog
from local_agent import LLMAgent

agent = LLMAgent("tiiuae/Falcon3-10B-Instruct")

promptTemplate: Final= """ 
You are an accurate and reliable AI assistant.
You will be given will be given chunks from a variety of scientific papers, representing their topmost and bottom most sections, as well as a question. The papers are preselected to provide as much information as possible for you to be able to answer the question on the basis of the information provided there. You can always assume that the information from the paper is trustworthy and logically correct. Use the information that is provided in the paper as ground truth to answer the question.

For your generated answer and for it to be easily verifiable it is also important to add context. Each claim that you make when answering the question must reference one or multiple documents, on whose information you justify making that claim.

Input Format:
Your input will have the following structure, repeated for each individual document, that you should use for answering:

"Document [DOCUMENT_NUMBER]" - "TITLE OF THE PAPER"
[TOP K chars]: "THE TEXT THAT MAKES UP THE TOP K CHARACTERS OF THE PAPER"
[BOTTOM L chars]: "THE TEXT THAT MAKES UP THE BOTTOM L CHARACTERS OF THE PAPER"

The document number provided here is the one that you must use for referencing the paper and justifying your answer.

Referencing instructions
Citations must be added after every claim that you make. Claims can span multiple sentences, but as soon as a factual statement is made, that statement must be justified by providing a reference for a paper. It is not sufficient have multiple claims consecutively and only providing one reference at the end of the sentence for all of them.  
Every sentence with a claim or factual statement must be justified with a reference that, that links to the document where the underlying information justifying the claim lies. 

You are only allowed to use The DOCUMENT_NUMBER for that reference, which is provided to you. 
You are only allowed to add multiple references if you cannot fully justify the claim using only the information in one document. In that case you are allowed to reference multiple to have full justification for your claim. If one document provides sufficient information for fully justifying the claim, you are only allowed to reference this one document for the claim.

Example for referencing instructions:
correct: 
-"Homomorphic encryption is an encryption method that enables computing over encrypted data[3]. This has a wide range of real world ramifications such as being able to blindly compute a search result sent to a remote server 
without revealing its content or the underlying query itself. In such a setting, a client can encrypt their data locally, send it to an untrusted third party for processing, and receive an encrypted result that can only be decrypted by the client[2]. (good because there are a number of references and sentences with the same claim share the same reference )

wrong:
-"Homomorphic encryption is an encryption method that enables computing over encrypted data. This has a wide range of real world ramifications such as being able to blindly compute a search result sent to a remote server 
without revealing its content or the underlying query itself. In such a setting, a client can encrypt their data locally, send it to an untrusted third party for processing, and receive an encrypted result that can only be decrypted by the client[2]. (wrong because there is only one reference for multiple claims made)



Referencing format
References are only allowed as the last part of a sentence, BEFORE it gets closed with ".", "?", "!" or any similar punctuation marks. The only allowed formats of  sentences are thus:
quoting a document: [TEXT THAT THE SENTENCE INCLUDES WITHOUT ANY REFERENCE TO DOCUMENTS][DOCUMENT REFERENCE][PUNCTUATION_MARK] and
not quoting a document: [TEXT THAT THE SENTENCE INCLUDES WITHOUT ANY REFERENCE TO DOCUMENTS][PUNCTUATION_MARK]

Examples for referencing format
Correct: 
-single reference: "Machine learning is a rapidly growing field [3]."
-multiple references: "Machine learning is rapidly growing field [3,4]."

wrong:
-"Machine learning is a rapidly growing field.[3]" (the reference is wrongly added after the sentence is already closed)
-"Machine learning is a rapidly growing field(3)." (references surrounded by wrong symbol) 
-"Machine learning is a rapidly growing field([3])." (references should only be surrounded by square brackets) 

In order to keep up with scientific quoting formatting, the references can only have the following format
single reference: "[X]"
multiple references: "[X,Y,Z,...]"

with square brackets being the only thing surrounding the DOCUMENT_NUMBER referenced and X,Y,Z, ... being the DOCUMENT NUMBER THAT is being referenced. 

Output Format:
The generated answer with the references. Do not add anything else to the respones, no reference section, no comments and no further explanations.

Documents: "{document_texts}"
Question: "{question}"
"""
  
document_texts="""Document [1] - "Title: On the complexity of cache analysis for different replacement policies":
Title: On the complexity of cache analysis for different replacement policies

[TOP 1031 chars]: On the complexity of cache analysis for different replacement policies
abstract: Modern processors use cache memory: a memory access that "hits" the cache returns early, while a "miss" takes more time. Given a memory access in a program, cache analysis consists in deciding whether this access is always a hit, always a miss, or is a hit or a miss depending on execution. Such an analysis is of high importance for bounding the worst-case execution time of safety-critical real-time programs.There exist multiple possible policies for evicting old data from the cache when new data are brought in, and different policies, though apparently similar in goals and performance, may be very different from the analysis point of view. In this paper, we explore these differences from a complexity-theoretical point of view. Specifically, we show that, among the common replacement policies, LRU (Least Recently Used) is the only one whose analysis is NP-complete, whereas the analysis problems for the other policies are PSPACE-complete.

Document [2] - "Title: Data Cache Prefetching with Perceptron Learning":
Title: Data Cache Prefetching with Perceptron Learning

[TOP 4939 chars]: Data Cache Prefetching with Perceptron Learning
abstract: Cache prefetcher greatly eliminates compulsory cache misses, by fetching data from slower memory to faster cache before it is actually required by processors. Sophisticated prefetchers predict next use cache line by repeating program's historical spatial and temporal memory access pattern. However, they are error prone and the mis-predictions lead to cache pollution and exert extra pressure on memory subsystem. In this paper, a novel scheme of data cache prefetching with perceptron learning is proposed. The key idea is a two-level prefetching mechanism. A primary decision is made by utilizing previous table-based prefetching mechanism, e.g. stride prefetching or Markov prefetching, and then, a neural network, perceptron is taken to detect and trace program memory access patterns, to help reject those unnecessary prefetching decisions. The perceptron can learn from both local and global history in time and space, and can be easily implemented by hardware. This mechanism boost execution performance by ideally mitigating cache pollution and eliminating redundant memory request issued by prefetcher. Detailed evaluation and analysis were conducted based on SPEC CPU 2006 benchmarks. The simulation results show that generally the proposed scheme yields a geometric mean of 60.64%-83.84% decrease in prefetching memory requests without loss in instruction per cycle(IPC)(floating between -2.22% and 2.55%) and cache hit rate(floating between -1.67% and 2.46%). Though it is possible that perceptron may refuse useful blocks and thus cause minor raise in cache miss rate, lower memory request count can decrease average memory access latency, which compensate for the loss, and in the meantime, enhance overall performance in multi-programmed workloads.
Introduction: Memory wall has become increasingly a bottleneck for processor performance boost , . Prefetching is a useful technique for addressing the memory wall issue for its unique advantage of not incurring large area and energy penalty . It effectively hide the gap between memory access latency and cache access latency. Ideally, a prefetcher can eliminate nearly all the cache misses, thus help achieve the execution performance close to that of a perfect cache , . Due to these advantages, prefetching is now being widely used in high-performance processors, for example, Intel Xeon , and IBM POWER , .However, prefetching is not panacea . In processor, prefetcher predicts 'next use' cache line and place it before required by the processor. Due to spatial and temporal predictability of memory access patterns , , sophisticated prefetching methods makes prediction by tracing the route to detect fixed stride between two consecutive memory references. It's indeed useful for regular data accesses, but error prone when facing irregular ones. Naive prefetching is harmful because it evicts the useful cache lines and brings useless cache lines, which leads to cache space consuming and performance degrading , also known as cache pollution.In order to ameliorate the problem, an operative way is to let prefetching learn from and adapt to the memory access pattern of the instruction stream. In this paper, a novel scheme of two-level prefetcher is proposed, which combine traditional methods(stride prefetching , , , , , Markov prefetching , etc.) with perceptron learning. The first level provides suggestion and necessary information, and the second level, our perceptron, will make final decision, according to the dynamically detected memory reference patterns.According to our simulation, this two-level prefetcher can bring significant cache level optimizations. Compared with traditional methods(stride prefetching and Markov prefetching), it cuts down a geometric mean of 60.64% and 83.84%, respectively, unnecessary memory request issued by prefetcher. Consequently, the pressure on next level cache or main memory largely mitigates due to the large decrease of unnecessary prefetching requests. Meanwhile, perceptron does not exert negative influence on hit rate (floating between -1.67% and 2.46%) and instruction per cycle(IPC)(floating between -2.22% and 2.55%).The key idea of our perceptron learning derives from Rosenblatt perceptron , , the simplest-structured artificial neural network, but just one neuron is used. The neuron has good learning ability for linear pattern classification task. The inputs of our perceptron are quantified features containing local and global, time and space history of instruction stream. The weights show highly sensitivity to each program's historical memory references behavior. In original method, the output of the perceptron is the dot product of the weights and a vector of inputs. In our work, the sum of input and weights product are computed, and the decision will be determined by the sign of result.This paper is organized into seven sections.

[BOTTOM 2967 chars]: However, compared with significant mitigate in cache pollution, minor cache hit rate degreed is tolerable. As mentioned above, with perceptron learning the quantity of prefetch suggestions comes from first level prefetcher decrease greatly, less information is provided for perceptron to make decision. Thus, this minor decrease in cache hit rate would be reasonable.
Conclusion and Future Work: This paper proposes a new scheme of two-level prefetching, with previous table-based prefetching works in the first level for giving suggestions along with providing necessary related information and perceptron learning works in the second level for making final decisions. Rather than using fixed pattern, perceptron learning, with local and global, time and space history, can dynamically detect and trace program's memory access pattern. What's more, this scheme is easily implemented by hardware, which does not 'steal' cycles from the execution of instruction stream.Our simulation shows that perceptron denies a large quantity of unnecessary memory request and thus, ameliorating cache pollution and mitigating memory traffic, while exert minor influence on IPC and cache hit rate. {{figure:9ce4e34f-9268-46cd-ba99-a130f372885e}}Some furthermore issues should be discussed here. Simulation shows a sharp decrease in first level prefetcher suggestions. As shown in Figure REF , our interpretation is that a feedback loop exists in cache hit and first level prefetcher suggestions. A higher cache hit rate results in infrequent cache misses, thus less new entries are pushed in GHB. Consequently, GHB updates less frequently. Meanwhile, inadequate real time information leads to less suggestions issued by first level prefetcher. This may detriment perceptron in making reasonable determination due to inferior panorama of memory reference records. Then decrease in perceptron accuracy further exert negative influence on cache hit rate. We refer to this as a dynamic equilibrium. And our scheme would push this dynamic equilibrium to a new point of balance with the achievement of better performance in addressing cache pollution and cutting down memory traffic.This paper makes the following contributions: 1) A novel two-level prefetcher are proposed, which can effectively reduce unnecessary memory requests issued by prefetcher. This is vital for multi-core systems; 2) It shows that perceptron learning can be used to exploit and trace program memory access pattern dynamically; 3) Perceptron learning combined with previous table-based mechanisms can achieve large performance increase; and 4) Simulation modeled for fully evaluation of our scheme in dimensions including IPC, memory requests quantities, prefetching accuracy, cache hit rate and etc. is taken. In future work, we intend to explore what is the best opportunity to trigger prefetcher. We also plan to explore how to maintain enough information in GHB for perceptron to make reasonable decisions.

Document [3] - "Title: Data access optimizations for highly threaded multi-core CPUs with multip...":
Title: Data access optimizations for highly threaded multi-core CPUs with multiple memory controllers

[TOP 4966 chars]: Data access optimizations for highly threaded multi-core CPUs with multiple memory controllers
abstract: Processor and system architectures that feature multiple memory controllers are prone to show bottlenecks and erratic performance numbers on codes with regular access patterns. Although such effects are well known in the form of cache thrashing and aliasing conflicts, they become more severe when memory access is involved. Using the new Sun UltraSPARC T2 processor as a prototypical multi-core design, we analyze performance patterns in low-level and application benchmarks and show ways to circumvent bottlenecks by careful data layout and padding.
The Sun UltraSPARC T2 processor: Trading high single core performance for a highly parallel single chip architecture is the basic idea of T2 as can be seen in Fig. REF : Eight simple in-order SPARC cores (running at 1.2 or 1.4 GHz) are connected to a shared, banked L2 cache and four independently operating dual channel FB-DIMM memory controllers through a non-blocking switch, thereby providing UMA access characteristics with scalable bandwidth. Such features were previously only available in shared-memory vector computers like the NEC SX series. To overcome the restrictions of in-order architectures and long memory latencies, each core is able to support up to eight threads, i.e. there are register sets, instruction pointers etc. to accommodate eight different machine states. There are two integer, two memory and one floating point pipeline per core. Although all eight threads can be interleaved across the floating point and memory pipes, each integer pipe is hardwired to a group of four threads. The CPU can switch between the threads in a group on a cycle-by-cycle basis, but only one thread per group is simultaneously active at any time. If a thread has to wait for resources like, e.g., memory references, it will be put in an inactive state until the resources become available which allows for effective latency hiding but restricts each thread to a single outstanding cache miss. Running more than a single thread per core is therefore mandatory for most applications, and thread placement (“pinning”) must be implemented. This can be done with the standard Solaris processor_bind() system call or, more conveniently but only available for OpenMP, using the SUNW_MP_PROCBIND environment variable. {{figure:ecf23b89-7c1d-45a2-82e7-5703d127224b}}Each memory controller is associated with two L2 banks. A very simple scheme is employed to map addresses to controllers and banks: Bits 8 and 7 of the physical memory address select the memory controller to use, while bit 6 determines the L2 bank , . Consecutive 64-byte cache lines are thus served in turn by consecutive cache banks and memory controllers. Due to the fact that typical page sizes are at least 4 kB the distinction between physical and virtual addresses is of no importance here.The aggregated nominal main memory bandwidth of 42 GB/s (read) and 21 GB/s (write) for a single socket is far ahead of most other general purpose CPUs and topped only by the NEC SX-8 vector series. Since there is only a single floating point unit (performing MULT or ADD operations) per core, the system balance of approximately 4 bytes/flop (assuming read) is the same as for the NEC SX-8 vector processor. [GH]In practiceIn our experience, as shown in Sect. REF , only about one third of the theoretical bandwidth can actually be measured.Beyond the requirements of the tests presented here one should be aware that the T2 chip also comprises on-chip PCIe-x8 and 10 Gb Ethernet connections as well as a cryptographic coprocessor. These features are reminiscent of the actual concept of the chip: It is geared towards commercial, database and typical server workloads. Consequently, one should not expect future versions to improve on HPC-relevant weaknesses of its design.
Benchmarks and optimizations: This section describes the benchmarks that were used to pinpoint aliasing effects, performance results and optimization techniques. All measurements were performed on a Sun SPARC Enterprise T5120 system running at 1.2 GHz.
McCalpin STREAM: The STREAM benchmark is a widely used code to assess the memory bandwidth capabilities of a single processor or shared memory computer system. It performs the OpenMP-parallel operationscopy: C(:)=A(:) scale: B(:)=s*C(:) add: C(:)=A(:)+B(:) triad: A(:)=B(:)+s*C(:)on double precision (DP) vectors A, B, and C at an array length that is large compared to all cache sizes. The standard Fortran code allows some variations as to how the data is allocated. If the arrays are put into a COMMON block, a configurable offset (“padding”) can be inserted so that their base addresses vary with the offset in a defined way:-3pt0.2cm0.2cm PARAMETER (N=20000000,offset=0, & ndim=N+offset,ntimes=10) DOUBLE PRECISION a(ndim),b(ndim),c(ndim) COMMON a,b,cPerformance results are reported as bandwidth numbers (GB/s).

[BOTTOM 2875 chars]: This could be eliminated by padding the first array dimension. Second, the sawtooth-like performance pattern is a “modulo effect” which emerges from [GH]{{formula:e7bce58c-2704-4f80-a7d3-be1a89852064}}{{formula:fea1cec0-e431-4489-8172-fc4bd2693f5e}} not being a multiple of the number of threads. A simple way to remove the pattern is to coalesce several outer loop levels in order to lengthen the OpenMP parallel loop. Results for up to 64 threads and two-way coalescing are also shown in Fig. REF and corroborate the call for extensions of the OpenMP standard towards more flexible options for parallel execution of loop nests.However, [GH]as Fig. REF shows,even when these optimizations are employed, the system falls short of [GH]this expectationthe performance expectations derived from STREAM by [GH]nearly a factor of [GH]two1.5. As for the reason one may speculate that the T2's arithmetic units are a limiting factor due to the rather low code balance of LBM of {{formula:a52cab73-fe4c-46d3-85c6-300cec03cebb}} bytes/flop[GH], so that the code is not memory-bound on this processor. This conclusion is supported by the observation that LBM performance does not change if the benchmark is carried out in single precision (the SPARC core's peak performance is identical for single and double precision). More cores or a larger peak performance per core should thus improve the results.Interestingly, comparing 32- and 64-thread performance in Figs. REF , REF and REF we conclude that the smaller the application balance in bytes/flop the larger the gain when using 64 instead of 32 threads. This is contrary to expectations as strongly memory-bound kernels should benefit from a larger number of outstanding references.
Conclusions: We have pinpointed aliasing conflicts when accessing memory on Sun's UltraSPARC T2 multi-core processor. Due to the simple mapping of memory controllers to physical addresses, bandwidth-intensive code tends to show large performance fluctuations with respect to problem size. Using explicit alignment and padding techniques we were able to remedy aliasing conflicts for a simple vector triad benchmark and a 2D Jacobi heat equation solver. For a D3Q19 lattice-Boltzmann algorithm we could show that an appropriate choice of data layout removes most of the aliasing. [GH]We believe these optimizations to be very relevant on large-scale systems because predictable one-node performance is essential for getting good parallel efficiency.[GH]In generalFinally one must emphasize that in the light of upcoming massively multi-core, multi-threaded designs, the rigid OpenMP programming model might not be the ultimate solution for shared-memory parallel programming in the future.
Acknowledgements: We wish to thank Rick Hetherington, Denis Sheahan and Ram Kunda from Sun Microsystems, and Samuel Williams from UCB for valuable discussions."""

question="How do modern compilers optimize memory access patterns to reduce cache misses?"

prompt = promptTemplate.format(document_texts=document_texts,question=question ) 

llmJudgement = agent.generate(prompt)
      
printAndLog(llmJudgement)     

         


